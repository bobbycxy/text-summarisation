{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarisation with Transformer from Scratch\n",
    "\n",
    "## 1.0. Introduction\n",
    "\n",
    "In today's digital age, news flows in an endless stream from various sources. We have great amount of news articles everyday. But, there are a small amount of useful information in the articles and it is hard to extract useful information manually. As a result, there are lots of news articles but, it is hard to read all of articles and find informative news manually. One of solutions on this problem is to summarize texts in the article.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src=\"https://blog.fpt-software.com/hs-fs/hubfs/image-8.png?width=376&name=image-8.png\" alt=\"Text Summarisation Visual\" />\n",
    "</p>\n",
    "\n",
    "### 1.1. Problem Statement\n",
    "Text summarisation automatically gives the reader a summary containing important sentences and relevant information about an article. This is highly useful because it shortens the time needed to capture the meaning and main events of an article. Broadly, there are 2 ways of performing text summarisation - abstractive and extractive. \n",
    "\n",
    "**Abstractive.** Abstractive methods analyse input texts and generate new texts that capture the essence of the original text. If trained correctly, they convey the same meaning as the original text, yet are more concise.\n",
    "\n",
    "**Extractive.** Extractive methods, on the other, take out the important texts from the original text and joins them to form a summary. Hence, they do not generate any new texts.\n",
    "\n",
    "In this assignment, we'll use the abstractive method to solve the following problem - **given a news article, can we return a succinct summary of the article?**\n",
    "\n",
    "### 1.2. Abstractive Text Summarisation\n",
    "Abstractive text summarisation can be achieved with transformer models. In the notebook titled 'text-summarisation-abs-pretrained-training-N.ipynb', we had used a pretrained T5 transformer model to abstract news summaries from news articles. Here, I will be building a transformer model from scratch and I will attempt to see how this transformer model can generate news summaries. \n",
    "\n",
    "The architecture of this model is going to reference the transformer architecture described in \"Attention is all you need\" [[PDF](https://arxiv.org/pdf/1706.03762.pdf)]. This transformer will have an encoder and decoder.\n",
    "\n",
    "Similar to the 'text-summarisation-abs-pretrained-training-N.ipynb', **we'll be using XSum.** \n",
    "\n",
    "**Why do we use the XSum dataset?** XSum stands for 'Extreme Summarisation' and it is a dataset for evaluating single-document summarisation systems. Each article summary follows the question of 'What is the article about?'. It comprises of 226,711 news articles accompanied with one-sentence summary, and they are collected from BBC (from 2010 to 2017) which cover a wide variety of genres such as general news, politics, sports, weather, business, technology, science, health, family, education, entertainment and arts. With a wide span of genre, it is the ideal dataset to use for our pre-trained models fine tuning exercise.\n",
    "\n",
    "### 1.3. Environment\n",
    "AWS EC2 Instance - Deep Learning AMI GPU TensorFlow 2.7.3 (Ubuntu 20.04). Instance type: c5.2xlarge\n",
    "\n",
    "(Learn how to set up your deep learning workstation with AWS [here](https://medium.com/@bobbycxy/detailed-guide-to-connect-ec2-with-vscode-2c084c265e36?source=your_stories_page))\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## 2.0. Data Preprocessing\n",
    "### 2.1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os \n",
    "import time\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Prepare Key Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER_LEN = 500\n",
    "DECODER_LEN = 100\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = BATCH_SIZE*8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Import Data\n",
    "\n",
    "As mentioned in Section 1.1., we will use the XSum dataset. This dataset is available with huggingface's datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"xsum\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.train_test_split(\n",
    "    train_size=0.1, test_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>summary</th>\n",
       "      <th>id</th>\n",
       "      <th>train test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13 September 2013 Last updated at 08:33 BST\\nO...</td>\n",
       "      <td>The glass ceiling is still a problem for women...</td>\n",
       "      <td>24075638</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The character is seen dying as a result of an ...</td>\n",
       "      <td>Marvel has killed off The Hulk's human alter e...</td>\n",
       "      <td>36781990</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The vaccine, which has been developed in India...</td>\n",
       "      <td>Health officials say they will begin the roll ...</td>\n",
       "      <td>11813238</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He said he had sent the US evidence of Fethull...</td>\n",
       "      <td>The Turkish Prime Minister, Binali Yildirim, h...</td>\n",
       "      <td>36833972</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Multiple Sclerosis Society's report sugges...</td>\n",
       "      <td>People with the most common form of multiple s...</td>\n",
       "      <td>34392429</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20399</th>\n",
       "      <td>They are the postal voters - and he is one of ...</td>\n",
       "      <td>\"For 20% of my patch, the election is over,\" s...</td>\n",
       "      <td>32405988</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20400</th>\n",
       "      <td>Her lawyer successfully argued that she might ...</td>\n",
       "      <td>A teenager who hurled abuse at child murderers...</td>\n",
       "      <td>36439890</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20401</th>\n",
       "      <td>It happened as the man cycled on the B800 Kirk...</td>\n",
       "      <td>A 78-year-old cyclist has died after being inv...</td>\n",
       "      <td>26219323</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20402</th>\n",
       "      <td>Mohammad Reza Mahdavi Kani was considered to b...</td>\n",
       "      <td>The head of the Assembly of Experts, the body ...</td>\n",
       "      <td>29685856</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20403</th>\n",
       "      <td>It wanted to \"restore dignity back to the stat...</td>\n",
       "      <td>South Africa's government has ordered sculptor...</td>\n",
       "      <td>25845603</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20404 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                document  \\\n",
       "0      13 September 2013 Last updated at 08:33 BST\\nO...   \n",
       "1      The character is seen dying as a result of an ...   \n",
       "2      The vaccine, which has been developed in India...   \n",
       "3      He said he had sent the US evidence of Fethull...   \n",
       "4      The Multiple Sclerosis Society's report sugges...   \n",
       "...                                                  ...   \n",
       "20399  They are the postal voters - and he is one of ...   \n",
       "20400  Her lawyer successfully argued that she might ...   \n",
       "20401  It happened as the man cycled on the B800 Kirk...   \n",
       "20402  Mohammad Reza Mahdavi Kani was considered to b...   \n",
       "20403  It wanted to \"restore dignity back to the stat...   \n",
       "\n",
       "                                                 summary        id train test  \n",
       "0      The glass ceiling is still a problem for women...  24075638      train  \n",
       "1      Marvel has killed off The Hulk's human alter e...  36781990      train  \n",
       "2      Health officials say they will begin the roll ...  11813238      train  \n",
       "3      The Turkish Prime Minister, Binali Yildirim, h...  36833972      train  \n",
       "4      People with the most common form of multiple s...  34392429      train  \n",
       "...                                                  ...       ...        ...  \n",
       "20399  \"For 20% of my patch, the election is over,\" s...  32405988      train  \n",
       "20400  A teenager who hurled abuse at child murderers...  36439890      train  \n",
       "20401  A 78-year-old cyclist has died after being inv...  26219323      train  \n",
       "20402  The head of the Assembly of Experts, the body ...  29685856      train  \n",
       "20403  South Africa's government has ordered sculptor...  25845603      train  \n",
       "\n",
       "[20404 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(raw_datasets['train'])\n",
    "train_df['train test'] = 'train'\n",
    "# test_df = pd.DataFrame(raw_datasets['test'])\n",
    "# test_df['train test'] = 'test'\n",
    "\n",
    "# df = pd.concat([train_df, test_df], axis = 0).reset_index(drop = True)\n",
    "df = pd.concat([train_df], axis = 0).reset_index(drop = True)\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.loc[0,'document'].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Prepare the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = df['document']\n",
    "summary = df['summary']\n",
    "article = article.apply(lambda x: '<SOS> ' + x + ' <EOS>')\n",
    "summary = summary.apply(lambda x: '<SOS> ' + x + ' <EOS>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Before we train our model, we need to pre-process our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r\"&.[1-9]+;\",\" \",text)\n",
    "    return text\n",
    "article = article.apply(lambda x: preprocess(x))\n",
    "summary = summary.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n",
    "oov_token = '<unk>'\n",
    "article_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\n",
    "article_tokenizer.fit_on_texts(article)\n",
    "summary_tokenizer.fit_on_texts(summary)\n",
    "inputs = article_tokenizer.texts_to_sequences(article)\n",
    "targets = summary_tokenizer.texts_to_sequences(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112523 27350\n"
     ]
    }
   ],
   "source": [
    "ENCODER_VOCAB = len(article_tokenizer.word_index) + 1\n",
    "DECODER_VOCAB = len(summary_tokenizer.word_index) + 1\n",
    "print(ENCODER_VOCAB, DECODER_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 02:22:25.946720: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\n",
    "targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\n",
    "inputs = tf.cast(inputs, dtype=tf.int64)\n",
    "targets = tf.cast(targets, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the Transformer Architecture\n",
    "\n",
    "### 4.1. Positional Encoding\n",
    "Training sequential models like RNNs meant that our inputs are fed into the model in an order / sequence. However, that is very time consuming and compute expensive. When training a transformer network, we leave the model to learn their positions not as integers, or a range of 0 to 1. Instead, we let them learn positional vector embeddings.\n",
    "\n",
    "The values of the sine and cosine equations are small enough that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted. The sum of the positional encoding and word embedding is then fed into the model. This then allows the Transformer network to attend to the relative positions of your input data. \n",
    "\n",
    "It it with these functions that we insert them into the positional encoding function - positional_encoding()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Masking\n",
    "There are 2 types of masks that are useful when building a Transformer Network - the padding mask and look-ahead mask. Both help the softmax computation give the appropriate weights to the words in your input sentence.\n",
    "\n",
    "#### 4.2.1. Padding Mask\n",
    "The padding mask is meant to mask out the zeros by setting them close to negative infinity. If we don’t mask it, then the zeros will affect the softmax calculation. For example [1,2,3,0,0] → [1,2,3,-1e9,-1e9]. This way, the zeros don’t affect the score.\n",
    "\n",
    "#### 4.2.2. Look-Ahead Mask\n",
    "In training, we will have access to the complete correct output for the training example. The look-ahead mask helps the model pretend it had correctly predicted a part of the output and see it, without looking ahead, it can correctly predict the next output. \n",
    "\n",
    "For example, if the expected correct output is [1,2,3] and we wanted to see if given the model correctly predicted the first value it could predict the second value, then we’d mask out the second and the third values. As such, we would input the masked sequence [1,-1e9,-1e9] and see if it could generate [1,2,-1e9]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Self-Attention via Scaled Dot Product Attention\n",
    "Self attention allows for the parallization in operations which speeds up training. We can implement it with the scaled dot product attention which takes in the quey and key and value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Multi-head Attention\n",
    "\n",
    "<p align='center'>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:1400/1*UxtH2qdJAmPP0F6dxiawUg.png\" />\n",
    "</p>\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer.\n",
    "\n",
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The scaled_dot_product_attention defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using tf.transpose, and tf.reshape) and put through a final Dense layer.\n",
    "\n",
    "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights\n",
    "    \n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Encoder and Decoder Layer\n",
    "\n",
    "<p align='center'>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:863/0*jKqypwGzmDv7KDUZ.png\" />\n",
    "</p>\n",
    "\n",
    "#### 4.5.1. Encoder Layer\n",
    "Each encoder layer consists of sublayers:\n",
    "\n",
    "1.   Multi-head attention (with padding mask) \n",
    "2.    Point wise feed forward networks. \n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2. Decoder Layer\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the output from the masked multi-head attention sublayer.\n",
    "3.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is LayerNorm(x + Sublayer(x)). The normalization is done on the d_model (last) axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 Encoder and Decoder\n",
    "The Encoder here will consist of 1) input embeddings, 2) positional encoding and 3) `num_layers` encoder layers.\n",
    "\n",
    "The Decoder here will consist of 1) output embeddings, 2) positional encoding and 3) `num_layers` decoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "        \n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Putting together the Transformer Model\n",
    "\n",
    "<p align='center'>\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:856/1*ZCFSvkKtppgew3cc7BIaug.png\" />\n",
    "</p>\n",
    "\n",
    "The transformer will consist of the encoder, decoder and a final linear layer that outputs the probabilities of the words. The output of the decoder is this input to this final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "### 5.1. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4 # number of layers for the encoder and decoder\n",
    "d_model = 128 # dimension of the model\n",
    "dff = 512 # dimension of the feed-forward network\n",
    "num_heads = 4 # number of heads for the multihead attention \n",
    "dropout_rate = 0.2 # for regularization\n",
    "EPOCHS = 15 # how many times to train over the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Preparing Custom Optimizer\n",
    "A learning rate schedule helps to modulate how the learning rate of your optimizer changes over time. From the 'Attention Is All You Need' paper, they used the following formula. Such a learning rate increases linearly for the first `warmup_steps` training steps, and decreases thereafter proportionally to the inverse square root of the step number.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src=\"https://i.stack.imgur.com/GQurA.png\" alt=\"Text Summarisation Visual\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Loss and Metrics\n",
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Training And Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=ENCODER_VOCAB,\n",
    "    target_vocab_size=DECODER_VOCAB,\n",
    "    pe_input=1000,\n",
    "    pe_target=1000,\n",
    "    rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# # if a checkpoint exists, restore the latest checkpoint.\n",
    "# if ckpt_manager.latest_checkpoint:\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            enc_padding_mask, \n",
    "            combined_mask, \n",
    "            dec_padding_mask\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 10.2292 Accuracy 0.0000\n",
      "Epoch 1 Batch 100 Loss 10.1242 Accuracy 0.0166\n",
      "Epoch 1 Batch 200 Loss 9.8911 Accuracy 0.0290\n",
      "Epoch 1 Batch 300 Loss 9.5489 Accuracy 0.0328\n",
      "Epoch 1 Loss 9.4792 Accuracy 0.0333\n",
      "Time taken for 1 epoch: 1349.9867615699768 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 8.2002 Accuracy 0.0333\n",
      "Epoch 2 Batch 100 Loss 7.8102 Accuracy 0.0374\n",
      "Epoch 2 Batch 200 Loss 7.5549 Accuracy 0.0438\n",
      "Epoch 2 Batch 300 Loss 7.4271 Accuracy 0.0495\n",
      "Epoch 2 Loss 7.4066 Accuracy 0.0506\n",
      "Time taken for 1 epoch: 1319.7803394794464 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 7.1571 Accuracy 0.0506\n",
      "Epoch 3 Batch 100 Loss 6.9521 Accuracy 0.0582\n",
      "Epoch 3 Batch 200 Loss 6.8520 Accuracy 0.0662\n",
      "Epoch 3 Batch 300 Loss 6.7568 Accuracy 0.0736\n",
      "Epoch 3 Loss 6.7386 Accuracy 0.0750\n",
      "Time taken for 1 epoch: 1317.3954560756683 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 6.4537 Accuracy 0.0750\n",
      "Epoch 4 Batch 100 Loss 6.3494 Accuracy 0.0820\n",
      "Epoch 4 Batch 200 Loss 6.2739 Accuracy 0.0884\n",
      "Epoch 4 Batch 300 Loss 6.2064 Accuracy 0.0943\n",
      "Epoch 4 Loss 6.1954 Accuracy 0.0953\n",
      "Time taken for 1 epoch: 1308.8407213687897 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 6.0686 Accuracy 0.0954\n",
      "Epoch 5 Batch 100 Loss 5.9341 Accuracy 0.1009\n",
      "Epoch 5 Batch 200 Loss 5.8858 Accuracy 0.1060\n",
      "Epoch 5 Batch 300 Loss 5.8370 Accuracy 0.1108\n",
      "Saving checkpoint for epoch 5 at checkpoints/ckpt-1\n",
      "Epoch 5 Loss 5.8273 Accuracy 0.1117\n",
      "Time taken for 1 epoch: 1308.6041009426117 secs\n",
      "\n",
      "Epoch 6 Batch 0 Loss 5.7552 Accuracy 0.1118\n",
      "Epoch 6 Batch 100 Loss 5.6219 Accuracy 0.1164\n",
      "Epoch 6 Batch 200 Loss 5.5738 Accuracy 0.1207\n",
      "Epoch 6 Batch 300 Loss 5.5298 Accuracy 0.1249\n",
      "Epoch 6 Loss 5.5226 Accuracy 0.1257\n",
      "Time taken for 1 epoch: 1305.0847024917603 secs\n",
      "\n",
      "Epoch 7 Batch 0 Loss 5.2579 Accuracy 0.1257\n",
      "Epoch 7 Batch 100 Loss 5.3307 Accuracy 0.1297\n",
      "Epoch 7 Batch 200 Loss 5.2923 Accuracy 0.1335\n",
      "Epoch 7 Batch 300 Loss 5.2537 Accuracy 0.1372\n",
      "Epoch 7 Loss 5.2442 Accuracy 0.1379\n",
      "Time taken for 1 epoch: 1307.3053605556488 secs\n",
      "\n",
      "Epoch 8 Batch 0 Loss 4.9683 Accuracy 0.1379\n",
      "Epoch 8 Batch 100 Loss 5.0753 Accuracy 0.1415\n",
      "Epoch 8 Batch 200 Loss 5.0372 Accuracy 0.1451\n",
      "Epoch 8 Batch 300 Loss 5.0006 Accuracy 0.1484\n",
      "Epoch 8 Loss 4.9945 Accuracy 0.1490\n",
      "Time taken for 1 epoch: 1307.8428683280945 secs\n",
      "\n",
      "Epoch 9 Batch 0 Loss 4.9032 Accuracy 0.1491\n",
      "Epoch 9 Batch 100 Loss 4.8313 Accuracy 0.1524\n",
      "Epoch 9 Batch 200 Loss 4.8018 Accuracy 0.1557\n",
      "Epoch 9 Batch 300 Loss 4.7683 Accuracy 0.1588\n",
      "Epoch 9 Loss 4.7617 Accuracy 0.1594\n",
      "Time taken for 1 epoch: 1309.5909533500671 secs\n",
      "\n",
      "Epoch 10 Batch 0 Loss 4.6570 Accuracy 0.1594\n",
      "Epoch 10 Batch 100 Loss 4.6116 Accuracy 0.1626\n",
      "Epoch 10 Batch 200 Loss 4.5808 Accuracy 0.1656\n",
      "Epoch 10 Batch 300 Loss 4.5533 Accuracy 0.1685\n",
      "Saving checkpoint for epoch 10 at checkpoints/ckpt-2\n",
      "Epoch 10 Loss 4.5477 Accuracy 0.1691\n",
      "Time taken for 1 epoch: 1308.3172399997711 secs\n",
      "\n",
      "Epoch 11 Batch 0 Loss 4.5168 Accuracy 0.1691\n",
      "Epoch 11 Batch 100 Loss 4.4126 Accuracy 0.1721\n",
      "Epoch 11 Batch 200 Loss 4.3809 Accuracy 0.1750\n",
      "Epoch 11 Batch 300 Loss 4.3527 Accuracy 0.1778\n",
      "Epoch 11 Loss 4.3480 Accuracy 0.1783\n",
      "Time taken for 1 epoch: 1309.2917795181274 secs\n",
      "\n",
      "Epoch 12 Batch 0 Loss 4.2952 Accuracy 0.1784\n",
      "Epoch 12 Batch 100 Loss 4.2202 Accuracy 0.1812\n",
      "Epoch 12 Batch 200 Loss 4.1895 Accuracy 0.1840\n",
      "Epoch 12 Batch 300 Loss 4.1640 Accuracy 0.1867\n",
      "Epoch 12 Loss 4.1598 Accuracy 0.1871\n",
      "Time taken for 1 epoch: 1307.525609254837 secs\n",
      "\n",
      "Epoch 13 Batch 0 Loss 4.1428 Accuracy 0.1872\n",
      "Epoch 13 Batch 100 Loss 4.0436 Accuracy 0.1899\n",
      "Epoch 13 Batch 200 Loss 4.0221 Accuracy 0.1925\n",
      "Epoch 13 Batch 300 Loss 3.9964 Accuracy 0.1951\n",
      "Epoch 13 Loss 3.9919 Accuracy 0.1956\n",
      "Time taken for 1 epoch: 1313.1360776424408 secs\n",
      "\n",
      "Epoch 14 Batch 0 Loss 3.9577 Accuracy 0.1956\n",
      "Epoch 14 Batch 100 Loss 3.8642 Accuracy 0.1982\n",
      "Epoch 14 Batch 200 Loss 3.8336 Accuracy 0.2009\n",
      "Epoch 14 Batch 300 Loss 3.8070 Accuracy 0.2035\n",
      "Epoch 14 Loss 3.8008 Accuracy 0.2040\n",
      "Time taken for 1 epoch: 1315.0915877819061 secs\n",
      "\n",
      "Epoch 15 Batch 0 Loss 3.8513 Accuracy 0.2040\n",
      "Epoch 15 Batch 100 Loss 3.6801 Accuracy 0.2066\n",
      "Epoch 15 Batch 200 Loss 3.6461 Accuracy 0.2093\n",
      "Epoch 15 Batch 300 Loss 3.6234 Accuracy 0.2119\n",
      "Saving checkpoint for epoch 15 at checkpoints/ckpt-3\n",
      "Epoch 15 Loss 3.6169 Accuracy 0.2124\n",
      "Time taken for 1 epoch: 1319.9844408035278 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "   \n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_article):\n",
    "    input_article = article_tokenizer.texts_to_sequences([input_article])\n",
    "    input_article = tf.keras.preprocessing.sequence.pad_sequences(input_article, maxlen=ENCODER_LEN, \n",
    "                                                                   padding='post', truncating='post')\n",
    "\n",
    "    encoder_input = tf.expand_dims(input_article[0], 0)\n",
    "\n",
    "    decoder_input = [summary_tokenizer.word_index['<sos>']]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(DECODER_LEN):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions, attention_weights = transformer(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            dec_padding_mask\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        if predicted_id == summary_tokenizer.word_index['<eos>']:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_article):\n",
    "    summarized = evaluate(input_article=input_article)[0].numpy()\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  \n",
    "    return summary_tokenizer.sequences_to_texts(summarized)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> 13 September 2013 Last updated at 08:33 BST\n",
      "One woman who has well and truly broken through that ceiling in one of the toughest industries around is Dr Marlene Kanga.\n",
      "She is the head of Engineers Australia and is in Singapore for a global industry conference.\n",
      "She spoke to Ali Moore about what inspired her to become an engineer. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> The glass ceiling is still a problem for women trying to rise through the leadership ranks in business. <EOS> \n",
      " Predicted Summary :  it's a good issue of the year for the uk and the uk the uk is a good issue\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[0],\"\\n Predicted Summary : \", summarize(article[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> The character is seen dying as a result of an arrow to the head from Hawkeye, his Avengers teammate, in the third issue of Civil War II.\n",
      "Banner has been the Hulk's alter ego since the character's creation in 1962.\n",
      "However, for the last year, readers have seen Banner medicating himself to keep his anger management issues under control.\n",
      "During that time, a Korean-American teenage genius named Amadeus Cho has taken over as the new human alter-ego of The Hulk.\n",
      "\"This is uncharted territory for us,\" Marvel's editor in chief Axel Alonso told the New York Daily News.\n",
      "\"Only two things are for certain: It will take a long, long time for our heroes to come to terms with his loss, and the circumstance surrounding his death will leave a huge scar on the superhero community.\"\n",
      "In the latest edition of Civil War II, Hawkeye is seen killing his friend on the belief Banner is about to turn into the Hulk and unleash massive death and destruction.\n",
      "Banner had recently asked him for a mercy killing in the event of any disaster.\n",
      "His death comes a week after Marvel announced Riri Williams, a 15-year-old African-American girl, will become the new Iron Man.\n",
      "Marvel has previously brought back characters after their apparent death - including Captain America and Spider-Man, who returned within a year of their demise.\n",
      "It is not yet clear whether Banner could return in a similar way, but Marvel indicated there were no plans for a return. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> Marvel has killed off The Hulk's human alter ego Bruce Banner in its latest comic. <EOS> \n",
      " Predicted Summary :  the us space agency nasa has said it is a similar to the us space agency nasa says\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[1],\"\\n Predicted Summary : \", summarize(article[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> The vaccine, which has been developed in India, costs less than fifty US cents a dose and clinical tests suggest it could offer protection for between 10 and 15 years.\n",
      "Seasonal epidemics of meningitis kill thousands in Africa every year.\n",
      "Vaccinations will start in Burkina Faso, then move to Niger and Mali.\n",
      "Officials say clinical trials of the new vaccine have shown it to be highly effective in protecting against Meningitis A - a form of meningitis which kills thousands of young people each year across a swathe of sub-Saharan Africa dubbed the \"meningitis belt\".\n",
      "The vaccine is similar in concept to the one used successfully in Britain to tackle Meningitis C.\n",
      "If all goes to plan, it will first be offered to anyone aged between one and 29 years across 25 African states from Senegal in the west to Somalia in the east.\n",
      "The drug was developed in India at the cost of the Bill & Melinda Gates Foundation, but much more money will be required to complete the initial vaccination programme - some $570 million according to World Health Organization officials.\n",
      "It is hoped the majority of this money will come from donor organisations, with the rest coming from the African states.\n",
      "Speaking at a press conference in London ahead of the official launch of the programme, Dr Marc LaForce of the Meningitis Vaccine Project, said the low cost of the new vaccine made it something Africa could afford.\n",
      "\"This vaccine costs less than 50 cents a dose,\" he said. \"When we first had discussions with Africans, they basically implored us 'please do not make a vaccine we cannot afford - that's worse than having no vaccine at all.'\"\n",
      "Meningitis A kills one in ten of those infected. Brain damage, deafness or learning disabilities can afflict those who survive.\n",
      "It is caused by the bacterium Neisseria meningitidis group A, which mostly attacks infants, children, and young adults. It accounts for ninety per cent of all meningitis epidemics in Africa.\n",
      "The outbreaks strike during the dry season. In 1997, in the worst epidemic on record, 25,000 people died.\n",
      "Vaccines which protect both against Meningitis A and other forms of the infection are already available but cost much more and only offer short-term protection.\n",
      "Dr Jean-Marie Okwo-Bele, Director of the WHO Department of Immunization, Vaccines and Biologicals, told the conference the vaccine represented a great achievement.\n",
      "\"Having this vaccine is a huge accomplishment in public health, because this will affect the lives of 450 million people who are at risk of this disease and who live in the very well known African meningitis belt.\"\n",
      "The vaccine has been developed by the Serum Institute of India Ltd, but the license for the drug will be retained by PATH, an international nonprofit organisation. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> Health officials say they will begin the roll out of a new meningitis vaccine for sub-Saharan Africa on 6 December. <EOS> \n",
      " Predicted Summary :  the uk is to be offered the ebola virus in the uk and the uk to be scrapped\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[2],\"\\n Predicted Summary : \", summarize(article[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> He said he had sent the US evidence of Fethullah Gulen's criminal activities - allegations the cleric denies - in support of an extradition bid.\n",
      "Mr Yildirim insisted that his country was governed by the rule of law.\n",
      "Thousands of soldiers, police and officials have been detained or sacked since Friday's coup attempt.\n",
      "President Recep Tayyip Erdogan has again refused to rule out reinstating the death penalty for coup plotters if it is approved by parliament.\n",
      "The EU has warned that such a move would see the end of accession talks to the bloc.\n",
      "For now, at least, that seems not to worry President Erdogan, who is seizing the opportunity to tighten his grip, reports the BBC's Turkey correspondent, Mark Lowen.\n",
      "Prime Minister Yildirim was speaking after meeting the leader of the main opposition CHP party.\n",
      "He warned people not to act out of a spirit of revenge in the wake of Friday's failed military takeover, saying that would be \"unacceptable\" but whoever had acted against the law would be punished.\n",
      "\"Today we need unity,\" he said.\n",
      "The interior ministry has dismissed almost 9,000 police officers as part of a purge of officials suspected of involvement in the coup attempt.\n",
      "That followed the arrest of 6,000 military personnel and suspension of almost 3,000 judges over the weekend.\n",
      "Many of those accused of involvement are closely linked to the ruling apparatus.\n",
      "Turkey's armed forces have been dealt a significant psychological blow by the attempted coup, and their prestige and status have been damaged.\n",
      "Turks had assumed that coups were a thing of the past.\n",
      "Not only does this latest plot come as a threat to the country's democracy but it has also stymied its ability to act in its key role as a provider of regional security.\n",
      "Read more\n",
      "Reports on Tuesday said President Erdogan's Air Force adviser, Lt Col Erkan Kivrak, had been detained at a hotel in the southern province of Antalya.\n",
      "According to Reuters news agency, 257 staff at Mr Yildirim's own office - some 10% of the total number - have also been removed from duty.\n",
      "More than two dozen generals, including former air force chief Gen Akin Ozturk, have been remanded in custody pending the setting of trial dates.\n",
      "Like Mr Gulen, Gen Ozturk denies any involvement.\n",
      "Mr Yildirim said action would be taken against Mr Gulen's supporters.\n",
      "\"I'm sorry but this parallel terrorist organisation will no longer be an effective pawn for any country,\" Mr Yildirim said, according to Reuters.\n",
      "\"We will dig them up by their roots so that no clandestine terrorist organisation will have the nerve to betray our blessed people again.\"\n",
      "Meanwhile, the UN urged Turkey to uphold the rule of law and defend human rights in its response to the attempted coup.\n",
      "In a statement, High Commissioner for Human Rights Zeid Ra'ad Al Hussein said the mass suspension or removal of judges was \"cause for serious alarm\". He expressed \"deep regret\" at suggestions the death penalty could be reinstated.\n",
      "According to official figures from the prime minister's office, Friday night's coup attempt left 232 people dead and 1,541 wounded. <EOS>\n"
     ]
    }
   ],
   "source": [
    "print(article[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Headline :  <SOS> The Turkish Prime Minister, Binali Yildirim, has vowed to purge supporters of an exiled cleric \"by the roots\" in the aftermath of the failed coup. <EOS> \n",
      " Predicted Summary :  the us president of the russian president donald trump has said he will be a coup in the capital kiev\n"
     ]
    }
   ],
   "source": [
    "print(\"Real Headline : \", summary[3],\"\\n Predicted Summary : \", summarize(article[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the new album of the year has been mocked after a rare sign of a landmark album of the year'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article1 = \"\"\"Get ready to look perfect as you're thinking out loud on Feb 16, 2024 at Ed Sheeran's concert. The Grammy Award-winning singer is heading to Singapore for a one-night show at the National Stadium. Plus, he's bringing along English singer Calum Scott as a guest.\n",
    "\n",
    "Tickets for the concert will cost between S$88 and S$488 and can be purchased via Ticketmaster and at SingPost outlets.\n",
    "\n",
    "If you signed up for a UOB card for Taylor Swift's concert and didn't cancel your membership afterward, here's some great news. UOB cardholders can enjoy a presale from 10am on Oct 27 till 9.59 am on Oct 29.\n",
    "\n",
    "A second presale will be held for KrisFlyer members from 10am on Oct 30 to 9.59am on Oct 31. To get in on this presale, KrisFlyer UOB credit and debit cardholders will need to subscribe to receive KrisFlyer and SIA Group promotional emails via their KrisFlyer account preferences. They will then receive a unique access code from KrisFlyer via email on Oct 27. \n",
    "\n",
    "Members who are not KrisFlyer UOB credit or debit cardholders can download Kris+, the SIA Group’s lifestyle rewards app and spend 150 miles between Oct 20 and 25 to redeem a unique access code. Do note that redemptions are limited to the first 110,000 customers.\n",
    "\n",
    "Alternatively, those with loads of miles to spare can opt to redeem Categories 1 to 4 concert tickets using their miles via KrisFlyer Experiences from Oct 30. Tickets from Categories 1 to 4 may be redeemed with 49,000; 38,000; 29,000 and 19,000 miles, respectively.\n",
    "\n",
    "General sale will commence from 11am on Oct 31.\"\"\"\n",
    "\n",
    "summarize(article1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "In this method, I wanted to build my own transformer model using the 'Attention Is All You Need' paper's architecture to perform text abstraction. \n",
    "\n",
    "However, a big restraint I faced was cost. My AWS EC2 instance of c5.2xLarge was apparently not big enough to train the transformer to encode article lengths of up to 1024 words. As a result, I was only able to design the transformer model to encode article lengths up to 500 words and decode 100 words for headers, with a transformer model that has 4 heads (for the multi head attention) and has 5 layers of the encoder and decoder blocks. \n",
    "\n",
    "This limited the model from getting all the needed inputs from an article text to perform the needed training on summary targets. In other words, the model was insufficient to begin training with. I could increase it but it will be costly. Hence, I have refrained from further training my model, and have excluded it from being used as a method of extraction in the `event-extraction-workbook.ipynb` and `event-extraction-script.py`.\n",
    "\n",
    "However, I have decided to still include this notebook into my submission because I wanted to highlight that I know how to build transformer models and am currently inhibited by cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
