{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarisation with Extraction Workbook\n",
    "\n",
    "## 1.0. Introduction\n",
    "\n",
    "In today's digital age, news flows in an endless stream from various sources. We have great amount of news articles everyday. But, there are a small amount of useful information in the articles and it is hard to extract useful information manually. As a result, there are lots of news articles but, it is hard to read all of articles and find informative news manually. One of solutions on this problem is to summarize texts in the article.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src=\"https://blog.fpt-software.com/hs-fs/hubfs/image-8.png?width=376&name=image-8.png\" alt=\"Text Summarisation Visual\" />\n",
    "</p>\n",
    "\n",
    "### 1.1. Problem Statement\n",
    "Text summarisation automatically gives the reader a summary containing important sentences and relevant information about an article. This is highly useful because it shortens the time needed to capture the meaning and main events of an article. Broadly, there are 2 ways of performing text summarisation - abstractive and extractive. \n",
    "\n",
    "**Abstractive.** Abstractive methods analyse input texts and generate new texts that capture the essence of the original text. If trained correctly, they convey the same meaning as the original text, yet are more concise.\n",
    "\n",
    "**Extractive.** Extractive methods, on the other, take out the important texts from the original text and joins them to form a summary. Hence, they do not generate any new texts.\n",
    "\n",
    "In this assignment, we'll use the abstractive method to solve the following problem - **given a news article, can we return a succinct summary of the article?**\n",
    "\n",
    "### 1.2. Extractive Text Summarisation\n",
    "In the field of text summarisation, the techniques used can be broadly classified into two categories - extraction and abstraction. **Extraction** techniques take out the important sentences or phrases from the original text and joins them to form a summary. This involves a ranking algorithm to assign scores to sentences or phrases based on a certain relevance to the overall meaning of the document. \n",
    "\n",
    "This workbook will be used to develop 2 forms of extractive methods for text summarisation - 1) Weighted Frequency-Based Approach, and 2) Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0. Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/bobbycxy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bobbycxy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filepath, substring1 = '<content>', substring2 = '</content>'):\n",
    "    '''\n",
    "    inputs:\n",
    "        filepath: file path to article\n",
    "        substring1: by default, it is \"<content>\"\n",
    "        substring2: by default, it is \"</content>\"\n",
    "    output:\n",
    "        res: the top N ranked sentences\n",
    "    '''\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        article = f.read()\n",
    "\n",
    "    idx1 = article.index(substring1)\n",
    "    idx2 = article.index(substring2)\n",
    "\n",
    "    res = article[idx1 + (len(substring1) - 1) + 1:idx2]\n",
    "    res = res.strip() \n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0. Create the Text Summarisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## summarisation extraction based on weighted freqencies\n",
    "def summarise_weight_freq(text, n = None, max_sentence_length = 25):\n",
    "    '''\n",
    "    inputs:\n",
    "        text: body of words\n",
    "        n: [int] number of sentences, [float and lesser than 1] percentage of sentences, [None] 15% of the sentences extracted\n",
    "        max_sentence_length: keep sentence in the text that have sentence lengths equal or lesser to this\n",
    "    output:\n",
    "        summary: the top N ranked sentences\n",
    "    '''\n",
    "\n",
    "    sentences = sent_tokenize(text) # tokenize text into a list of sentences\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "\n",
    "    # In order to rank sentences by frequency, we need to have the word frequencies.\n",
    "    words = [word.lower() for word in word_tokenize(text) if word.lower() not in stop_words and word.isalnum()]\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    # calculate the sentence scores via weighted word frequencies\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        sentence_words = [word.lower() for word in word_tokenize(sentence) if word.lower() not in stop_words and word.isalnum()]\n",
    "        sentence_score = sum([word_freq[word] for word in sentence_words])\n",
    "        if len(sentence_words) <= max_sentence_length:\n",
    "            sentence_scores[sentence] = sentence_score/len(sentence_words) # calculates the average of the sum of word frequencies per sentence\n",
    "\n",
    "    # get the top n sentences\n",
    "    if n == None:\n",
    "        n = int(0.15 * len(sentences)) # rounds down to approximately 15% of the original sentence\n",
    "    elif isinstance(n,float) and n <= 1:\n",
    "        n = int(n * len(sentences))\n",
    "    summary_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse = True)[:n]\n",
    "    summary = ' '.join(summary_sentences)\n",
    "\n",
    "    return summary\n",
    "\n",
    "## summarisation extraction based on tf-idf\n",
    "def summarise_tfidf(text, n = None):\n",
    "    '''\n",
    "    inputs:\n",
    "        text: body of words\n",
    "        n: [int] number of sentences, [float and lesser than 1] percentage of sentences, [None] 15% of the sentences extracted\n",
    "    output:\n",
    "        summary: the top N ranked sentences\n",
    "    '''\n",
    "\n",
    "    sentences = sent_tokenize(text) # tokenize text into a list of sentences\n",
    "\n",
    "    # prepare a TF-IDF matrix using sklearn library\n",
    "    vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # calculate the cosine similarity of each sentence against the whole text\n",
    "    sentence_scores = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])[0]\n",
    "\n",
    "    # get the top n sentences\n",
    "    if n == None:\n",
    "        n = int(0.15 * len(sentences)) # rounds down to approximately 15% of the original sentence\n",
    "    elif isinstance(n,float) and n <= 1:\n",
    "        n = int(n * len(sentences))\n",
    "    summary_sentences = nlargest(n, range(len(sentence_scores)), key = sentence_scores.__getitem__)\n",
    "    summary = ' '.join([sentences[i] for i in sorted(summary_sentences)])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0. Testing out the functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Weighted Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTED FREQUENCY:\n",
      "article1.txt \n",
      " ['A second presale will be held for KrisFlyer members from 10am on Oct 30 to 9.59am on Oct 31.', 'They will then receive a unique access code from KrisFlyer via email on Oct 27.', 'UOB cardholders can enjoy a presale from 10am on Oct 27 till 9.59 am on Oct 29.']\n",
      "article2.txt \n",
      " ['Dozens of Palestinians have been killed in the West Bank in the latest flare-up of Israeli-Palestinian violence.', 'Oct 19 (Reuters) - Three Palestinians, including two teenagers, were killed by Israeli forces in separate incidents in the occupied West Bank early on Thursday, Palestinian official news agency WAFA said.', 'Israeli forces have carried out their fiercest bombardment of Gaza in response, killing more than 3,000 Palestinians and imposing a total siege on the blockaded enclave that Hamas controls, fuelling anger among Palestinians in the West Bank.']\n",
      "article3.txt \n",
      " ['He was driving along Sophia Road towards Upper Wilkie Road shortly before 11.30pm when he spotted a police roadblock.', 'The prosecutor said: “The accused requested his front-seat passenger to swop seats with him, so that he would not be presented as the driver of the vehicle at the roadblock.', 'SINGAPORE – In an attempt to evade arrest, a doctor who drove a car after drinking beer tried to change seats with his passenger when he spotted a police roadblock.']\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "\n",
    "print('WEIGHTED FREQUENCY:')\n",
    "\n",
    "for article in ['article1.txt','article2.txt','article3.txt']:\n",
    "    print(article, '\\n', sent_tokenize(summarise_weight_freq(preprocess(article), n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF:\n",
      "article1.txt \n",
      " ['UOB cardholders can enjoy a presale from 10am on Oct 27 till 9.59 am on Oct 29.', 'A second presale will be held for KrisFlyer members from 10am on Oct 30 to 9.59am on Oct 31.', 'They will then receive a unique access code from KrisFlyer via email on Oct 27.']\n",
      "article2.txt \n",
      " ['Oct 19 (Reuters) - Three Palestinians, including two teenagers, were killed by Israeli forces in separate incidents in the occupied West Bank early on Thursday, Palestinian official news agency WAFA said.', 'Dozens of Palestinians have been killed in the West Bank in the latest flare-up of Israeli-Palestinian violence.', 'Israel is preparing a ground assault in the Gaza Strip in response to a deadly attack by Palestinian militant group Hamas that killed at least 1,400 Israelis, mostly civilians, on Oct. 7.']\n",
      "article3.txt \n",
      " ['SINGAPORE – In an attempt to evade arrest, a doctor who drove a car after drinking beer tried to change seats with his passenger when he spotted a police roadblock.', 'The passenger refused to do so and Nah Kwang Meng, who practises at Dr Nah & Lee Family Clinic in Woodlands, initially failed a breathalyser test after he stepped out of the vehicle.', 'Assistant Public Prosecutor Chye Jer Yuan told the court that before going behind the wheel on July 14, 2022, Nah had dinner and consumed about three to four glasses of beer.']\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "\n",
    "print('TFIDF:')\n",
    "\n",
    "for article in ['article1.txt','article2.txt','article3.txt']:\n",
    "    print(article, '\\n', sent_tokenize(summarise_tfidf(preprocess(article), n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0. Conclusion\n",
    "Extractive text summarisation offers a higher accuracy, lower computation complexity and a better conservation of the information from the article compared to abstractive text summarisation. In the README.docx, I analyse the printed results of each method.\n",
    "\n",
    "In a next iteration, we can explore using graph-based ranking algorithms like TextRank. Text rank works by constructing a graph where sentences are represented as nodes, and the edges represent the relationships between them. The ranking score is determined by iteratively updating the scores of the sentences based on 1) the similarity and 2) importance of their neighboring sentences. The highly ranked sentences are then used to generate a summary of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
